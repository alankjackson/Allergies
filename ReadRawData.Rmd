---
title: "Read in raw data"
author: "Alan Jackson"
date: "April 14, 2019"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library("stringr") # string tools
library("readxl") # read excel files
library("lettercase") # fiddle with letter case
library("lubridate") # handle dates
library(RCurl)

knitr::opts_chunk$set(echo = TRUE)
```




```{r read pollen data}

path <- "https://www.houstontx.gov/health/Pollen-Mold/Pollen_Archives/"

#   Read in files using month names

#   Build a list of valid url's

url_list <- tribble(~url)

for (yr in as.character(2013:2019)) {
  for (mon in tolower(month.name)) {
    if (yr=="2013" & mon=="february") {mon <- "febraury"}
    url <- paste0(mon, "_", yr, "_pollen.xls")
    if (yr=="2018" | yr=="2019" |
        (yr=="2017"&(mon=="november"|mon=="december"))) {
          url <- paste0(mon, "_", yr, "_pollen.xlsx")
    }
    if(yr=="2018" & mon=="june") {url <- paste0(mon, "_", yr, "_pollen.xls")} 
    if(yr=="2018" & mon=="march") {next} # bad file lurking out there
    if (!url.exists(paste0(path,url))) {print(paste(url, "does not exist"))
                            next}
    #   add to url_list
    url_list <- add_row(url_list, url=url)
  }
}

#   Read in files using numeric months

for (yr in as.character(2013:2019)) {
  for (mon in sprintf("%02d", 1:12)) {
    url <- paste0(yr, mon, "-pollen-count.xls")
    if (grepl("201902", url)) {url <- paste0(url, "x")}
    if (!url.exists(paste0(path,url))) {print(paste(url, "does not exist"))
                            next}
    #   add to url_list
    url_list <- add_row(url_list, url=url)
  }
}

####################################
# now let's read the files and save
####################################

#   First read the urls into local files

for (url in url_list[,1]){
  download.file(paste0(path, url), destfile=url, mode="wb")
}

df <- tribble()
for (url in url_list$url){
  print(url)
  fileout <- paste0(url,".rds")
  df2 <- read_excel(url, col_names=FALSE)  
  saveRDS(df2, fileout)
}

```

##    Serious cleanup time

```{r cleanup and consolidate}

keep <- url_list

clean = function(data) {

  # Recode column names
  names.row = grep("DATE", df1[, 1, drop=TRUE])
  recode_vals = translate$to %>% set_names(translate$from)
  old_names = unlist(df1[names.row, ][-1]) 
  names(data) = c("Date", recode(old_names, !!!recode_vals))
  
  # Get Month and Year for dates
  mon <- str_remove(data[1,]$Date, "Month:\\s*")
  mon <- match(mon, toupper(month.name))
  yr  <- str_remove(data[2,]$Date, "YEAR:\\s*")

  # Remove Month, Year, Date and Total rows
  data = data[!grepl("Month|YEAR|DATE|Total", data$Date), ]
  
  # Change Date column to correct dates
  data$Date = paste(yr, mon, data$Date, sep="-")
  data$Date = lubridate::ymd(data$Date)
  
  data
}


translate <- tribble(
  ~from, ~to,
"Ashe Juniper / Bald Cypress",  "Ashe_JuniperOrBald_Cypress", 
"Alnus(Alder)",                 "Alnus",
"Black Gum",                    "Black_Gum", 
"Black Walnut",                 "Black_Walnut", 
"Cotton Wood",                  "Cotton_Wood",
"Glandular Mesquite",           "Glandular_Mesquite", 
"Osage Orange",                 "Osage_Orange", 
"Sweet Gum",                    "Sweet_Gum", 
"Gingko Biloba",                "Gingko_Biloba",  
"Burweed / Marshelder",         "BurweedOrMarshelder", 
"Dog Fennel",                   "Dog_Fennel", 
"Lamb's Quarters",              "Lambs_Quarters", 
"Partridge Pea",                "Partridge_Pea", 
"Plum Grannet",                 "Plum_Grannet", 
"Wild Carrot",                  "Wild_Carrot" 
)


map_df(list(df1, df2), clean)



  #saveRDS(df, filein)




```

